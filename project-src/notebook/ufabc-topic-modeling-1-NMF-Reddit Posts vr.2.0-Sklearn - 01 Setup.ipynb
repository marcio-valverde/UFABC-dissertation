{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **UFABC** \n",
    "\n",
    "### Mestrado em Engenharia da Informação\n",
    "\n",
    "### Modelagem de Tópicos para Análise de Indícios de Depressão em Postagens do Reddit na Língua Portuguesa\n",
    "\n",
    "* Aluno: Márcio Valverde\n",
    "* Orientador: Prof. Andre Takahata\n",
    "* Participação especial: Miro Neuro-Psicólogo (Neuropax)\n",
    "\n",
    "\n",
    "### Uma Abordagem com  **Fatoração de Matrizes Não-Negativas (Non-Negative Matrix Factorization - NMF)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ufabc-topic-modeling-1-NMF-Reddit Posts vr.2.0-Sklearn.ipynb\n",
    "    \n",
    "*(sklearn)* https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../image/Topics.png\" alt=\"image\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline**\n",
    "````\n",
    "   1. Reading Data\n",
    "    2. Pre-Processing\n",
    "       2.1 Lematization\n",
    "       2.2 Stematization\n",
    "       2.3 Stop words\n",
    "       2.4 Tokenization\n",
    "       2.5 Data Clean\n",
    "       2.6 Remoção da existência de Linhas vazias ou duplicadas\n",
    "       2.7 Extrai as etiquetas (ou anotações) de cada token no documento analisado\n",
    "       2.8 Processamento de Bigramas e Trigramas\n",
    "    3. EDA - Exploratory Data Analysis (Text Data)\n",
    "       3.1 Details of the resulting dataframe\n",
    "       3.2 Frequency of words\n",
    "       3.3 Top words com maior frequência\n",
    "       3.4 Word Cloud\n",
    "       3.5 Unique words\n",
    "       3.6 Descriptive statistics of the words\n",
    "       3.7 Histogram\n",
    "       3.8 Boxplot\n",
    "       3.9 Outliers\n",
    "    4. Processing\n",
    "       4.1 Automatically select the Best Number of Topics\n",
    "       4.2 Compute Coherence Score\n",
    "       4.3 TF-IDF\n",
    "       4.4 Hyper-parameters for NMF Modelling\n",
    "       4.5 NMF Model Training\n",
    "    5. Results\n",
    "       5.1 Words with high values of component for each topic\n",
    "       5.2 List of Topics\n",
    "       5.3 Manual Topic Labeling\n",
    "    6. Visualization of Results\n",
    "       6.1 List of documents\n",
    "       6.2 Find the most representative documents for each topic\n",
    "       6.3 Plotting the graphs for each Topic\n",
    "       6.4 Word cloud\n",
    "    7. Analysis of the results\n",
    "       7.1 Residue analysis\n",
    "       7.2 t-Distributed Stochastic Neighbor Embedding - Visualization\n",
    "       7.3 Residue analysis\n",
    "    8. Final Result list\n",
    "    9. Predict topic for new posts\n",
    "    10. Conclusion\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "current_time = time()\n",
    "print(\"Starting time: %0.3fs.\" % (time() - current_time))\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "\n",
    "import numpy as np\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib: {}'.format(plt.get_backend()))\n",
    "\n",
    "import seaborn as sns\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# modeling\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "\n",
    "\n",
    "import gensim\n",
    "print('gensim: {}'.format(gensim.__version__))\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "# text processing\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "print('nltk: {}'.format(nltk.__version__))\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import bigrams, trigrams\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    " # text cleaning \n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup \n",
    "import unidecode\n",
    "\n",
    "# spacy - lematização\n",
    "import spacy \n",
    "print('spacy: {}'.format(spacy.__version__))\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "'''  \n",
    "%pip install spacy\n",
    "pip install -U spacy\n",
    "python -m spacy download pt_core_news_lg\n",
    "'''\n",
    "#%pip install wordcloud\n",
    "\n",
    "# My libs\n",
    "import my_data_clean\n",
    "import my_save_file\n",
    "import my_date_time\n",
    "import my_nltk_commons\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data obtained from the extraction of posts on the social media platform Reddit. These posts were selected based on specific subreddits and keywords that may indicate the presence of symptoms related to depression.\n",
    "\n",
    "We will start with a brief exploratory data analysis to familiarize ourselves with the dataset in question.\n",
    "\n",
    "For this study, we have a dataset composed of 'n' posts.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/corpus/'\n",
    "input_file = 'pre-corpus.csv'\n",
    "df = pd.read_csv(path+input_file)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has the following structure:\n",
    "\n",
    "- post_id: A unique identifier for each post on a platform such as a forum or social network.\n",
    "- author: The name or identification of the author who created the post.\n",
    "- subreddit: The subreddit or category to which the post belongs on a platform like Reddit.\n",
    "- created_utc: The date and time of post creation in UTC (Coordinated Universal Time) format.\n",
    "- url: The URL associated with the post, if applicable.\n",
    "- self_text: The content of the post, which can be text written by the author.\n",
    "- title: The post's title.\n",
    "- texto: Another text field associated with the post, which may contain additional information.\n",
    "- keywords: Relevant keywords or terms found in the post.\n",
    "- link_flair_text: Text describing the post's category or classification.\n",
    "- score: The post's score or rating, which can be determined by user votes, for example.\n",
    "- num_comments: The number of comments associated with the post.\n",
    "- upvote_ratio: The ratio of upvotes to the total number of votes.\n",
    "- timestamp: A more human-readable representation of date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column containing the text to be CLEANED AND PROCESSED\n",
    "coluna = 'texto'\n",
    "\n",
    "# Column containing the final corpus (texts processed)\n",
    "coluna_corpus = 'corpus_text'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
